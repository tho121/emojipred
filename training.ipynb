{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import emoji\n",
    "from torch import torch, nn\n",
    "import seaborn as sns\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the trained tokenizer and add our own special tokens [USR], [LNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-cased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"[USR]\", \"[LNK]\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reduces chains of the same token down to just one instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeat_tokens(tweet_text):\n",
    "    words = tweet_text.split(' ')\n",
    "\n",
    "    for i in range(len(words) - 2, -1, -1):\n",
    "        if words[i] == \"[USR]\" and words[i+1] == \"[USR]\":\n",
    "            words.pop(i+1)\n",
    "\n",
    "        if words[i] == \"[LNK]\" and words[i+1] == \"[LNK]\":\n",
    "            words.pop(i+1)\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load processed text segments and remove those that, when encoded, have a token size less than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tho121/CMPT413/project/venv/lib/python3.8/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111809\n",
      "70368\n",
      "118723\n",
      "88681\n",
      "73788\n",
      "88274\n",
      "63875\n",
      "70085\n",
      "113561\n"
     ]
    }
   ],
   "source": [
    "emoji_list = ['joy',\n",
    "            'heart',\n",
    "            'sob',\n",
    "            'heart_eyes',\n",
    "            #'recycle',\n",
    "            'hearts',\n",
    "            'blush',\n",
    "            'two_hearts',\n",
    "            'kissing_heart',\n",
    "            'pensive',\n",
    "            ]\n",
    "\n",
    "df_list = []\n",
    "sizes = []\n",
    "min_size = 999999\n",
    "\n",
    "for emj in emoji_list:\n",
    "    filename = \"data/\" + emj + \"_no_media.csv\"\n",
    "\n",
    "    df = pd.read_csv(filename, sep='^([^,]+),')\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    df.text = df.apply(lambda row: remove_repeat_tokens(row.text), axis=1)\n",
    "\n",
    "    #remove super short tweets\n",
    "    df = df.drop(df[ df.text.map(lambda x: len(tokenizer.encode(x))) < 2 ].index)\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "    size = len(df.index)\n",
    "    print(size)\n",
    "    sizes.append(size)\n",
    "\n",
    "    if size < min_size:\n",
    "        min_size = size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the training dataset by randomly sampling from the top emojis, using the smallest included dataset size as the minimum value. Alternatively, set the minimum size to overwrite it, such as for parameter exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji\n",
      "😂    3000\n",
      "dtype: int64\n",
      "emoji\n",
      "😭    3000\n",
      "dtype: int64\n",
      "emoji\n",
      "😍    3000\n",
      "dtype: int64\n",
      "emoji\n",
      "😊    3000\n",
      "dtype: int64\n",
      "emoji\n",
      "😔    3000\n",
      "dtype: int64\n",
      "       emoji                                               text\n",
      "2020       😂                                      [USR] Aaaaaah\n",
      "11951      😂  [USR] You gotta be trolling fella. I don't giv...\n",
      "52625      😂  [USR] Worst thing is that we won’t be able to ...\n",
      "40055      😂              [USR] That’s what I was thinking! Lol\n",
      "107937     😂  [USR] A free market of “digital gold” controll...\n",
      "...      ...                                                ...\n",
      "87823      😔  [USR] Have a friend with residents who were wa...\n",
      "27450      😔  [USR] jealous that you skipped the Max Medina ...\n",
      "23310      😔            [USR] i think i just never works for me\n",
      "7716       😔                                [USR] It's shameful\n",
      "80391      😔  If I were given the chance I would go as this ...\n",
      "\n",
      "[15000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 5     #max is 9\n",
    "indices = sorted(range(len(sizes)), key=lambda i: sizes[i])[-num_classes:]\n",
    "min_size = sizes[indices[-num_classes]]\n",
    "\n",
    "#TEMP, set size for parameter exploration, else comment out\n",
    "min_size = 3000\n",
    "\n",
    "samples = []\n",
    "\n",
    "for i, df in enumerate(df_list):\n",
    "    if i in indices:\n",
    "        df_samples = df.sample(n=min_size)\n",
    "        print(df_samples.groupby(['emoji']).size())\n",
    "        samples.append(df_samples)\n",
    "\n",
    "df = pd.concat(samples)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>😂</td>\n",
       "      <td>[USR] So he can stay alive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>😂</td>\n",
       "      <td>[USR] Yeah, poor Hayato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>😂</td>\n",
       "      <td>[USR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>😂</td>\n",
       "      <td>[USR] It’s more funny when you see other men h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>😂</td>\n",
       "      <td>[USR] Oh Acha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>😔</td>\n",
       "      <td>I Haven’t Had Sex Since Sex Had Me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>😔</td>\n",
       "      <td>[USR] LOL damn yeah u got me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>😔</td>\n",
       "      <td>[USR] Ummmm…no words.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>😔</td>\n",
       "      <td>[USR] I nearly placed that bet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>😔</td>\n",
       "      <td>november tanpa win</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emoji                                               text\n",
       "0         😂                         [USR] So he can stay alive\n",
       "1         😂                            [USR] Yeah, poor Hayato\n",
       "2         😂                                              [USR]\n",
       "3         😂  [USR] It’s more funny when you see other men h...\n",
       "4         😂                                      [USR] Oh Acha\n",
       "...     ...                                                ...\n",
       "14995     😔                 I Haven’t Had Sex Since Sex Had Me\n",
       "14996     😔                       [USR] LOL damn yeah u got me\n",
       "14997     😔                              [USR] Ummmm…no words.\n",
       "14998     😔                     [USR] I nearly placed that bet\n",
       "14999     😔                                 november tanpa win\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "emoji\n",
      "😂    3000\n",
      "😊    3000\n",
      "😍    3000\n",
      "😔    3000\n",
      "😭    3000\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tho121/CMPT413/project/venv/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='emoji', ylabel='count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.groupby(['emoji']).ngroups)\n",
    "print(df.groupby(['emoji']).size())\n",
    "sns.countplot(df.emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the emojis to intergers for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df.emoji = label_encoder.fit_transform(df.emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the sample with the largest number of tokens, all other tokens will be padded to fit this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df.text.to_numpy()\n",
    "max_token_len = 0\n",
    "for tweet in tweets:\n",
    "    tokenized_tweet = tokenizer.encode(tweet)\n",
    "    size = len(tokenized_tweet)\n",
    "    if size > max_token_len:\n",
    "        max_token_len = size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_token_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the custom dataloader class that converts the samples to input for the DistilBert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EmojiPredictionDataset(Dataset):\n",
    "\n",
    "  def __init__(self, tweets, targets, tokenizer, max_len):\n",
    "\n",
    "    self.tweets = tweets\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.tweets)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "\n",
    "    tweet = str(self.tweets[item])\n",
    "    target = self.targets[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      tweet,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "      truncation=True,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'tweet_text': tweet,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataloader for the train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "\n",
    "  ds = EmojiPredictionDataset(\n",
    "    tweets=df.text.to_numpy(),\n",
    "    targets=df.emoji.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = max_token_len + 1\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the class that uses the DistilBert model and attach a classification head. Be sure to resize the token embedding size since they were modified to include the [USR] and [LNK] tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes, tokenizer):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "\n",
    "    bert_model = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    bert_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #freeze bert model\n",
    "    #for param in bert_model.base_model.parameters():\n",
    "    #    param.requires_grad = False\n",
    "\n",
    "    self.bert = bert_model\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "\n",
    "    #_, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "    distilbert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    hidden_state = distilbert_output[0]                    \n",
    "    pooled_output = hidden_state[:, 0]\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free up some memory cause I'm running on a laptop with 6 GB of VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(df.groupby(['emoji']).ngroups, tokenizer)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 136])\n",
      "torch.Size([16, 136])\n"
     ]
    }
   ],
   "source": [
    "input_ids = data['input_ids'].cuda()\n",
    "attention_mask = data['attention_mask'].cuda()\n",
    "\n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCHS = 4\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training function per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, scheduler, n_examples):\n",
    "\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "\n",
    "    input_ids = d[\"input_ids\"].cuda()\n",
    "    attention_mask = d[\"attention_mask\"].cuda()\n",
    "    targets = d[\"targets\"].cuda()\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define evaluation function per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, n_examples):\n",
    "\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for d in data_loader:\n",
    "\n",
    "      input_ids = d[\"input_ids\"].cuda()\n",
    "      attention_mask = d[\"attention_mask\"].cuda()\n",
    "      targets = d[\"targets\"].cuda()\n",
    "\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      loss = loss_fn(outputs, targets)\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training. After each epoch, the best model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "----------\n",
      "Train loss 1.3068583504566085 accuracy 0.4614074074074074\n",
      "Val   loss 1.2588843272087422 accuracy 0.496\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "Train loss 1.0940199077694337 accuracy 0.5706666666666667\n",
      "Val   loss 1.2666435875791184 accuracy 0.484\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "Train loss 0.9701792399677055 accuracy 0.6304444444444445\n",
      "Val   loss 1.311183879984186 accuracy 0.48666666666666664\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "Train loss 0.8926940796217082 accuracy 0.6657037037037037\n",
      "Val   loss 1.3657185295794874 accuracy 0.48666666666666664\n",
      "\n",
      "CPU times: user 9min 12s, sys: 2min 17s, total: 11min 30s\n",
      "Wall time: 11min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = {'train_acc': [],\n",
    "           'train_loss': [],\n",
    "           'val_acc': [],\n",
    "           'val_loss': []}\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, scheduler, len(df_train))\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, len(df_val))\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state_quick.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4960, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_acc': [tensor(0.4614, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.5707, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.6304, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.6657, device='cuda:0', dtype=torch.float64)],\n",
       " 'train_loss': [1.3068583504566085,\n",
       "  1.0940199077694337,\n",
       "  0.9701792399677055,\n",
       "  0.8926940796217082],\n",
       " 'val_acc': [tensor(0.4960, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.4840, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.4867, device='cuda:0', dtype=torch.float64),\n",
       "  tensor(0.4867, device='cuda:0', dtype=torch.float64)],\n",
       " 'val_loss': [1.2588843272087422,\n",
       "  1.2666435875791184,\n",
       "  1.311183879984186,\n",
       "  1.3657185295794874]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5133, device='cuda:0', dtype=torch.float64)\n",
      "1.2723050878403035\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = eval_model(model, test_data_loader, loss_fn, len(df_test))\n",
    "print(test_acc)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to use for getting the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(model, data_loader):\n",
    "\n",
    "  model = model.eval()\n",
    "\n",
    "  all_targets = []\n",
    "  all_preds = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for d in data_loader:\n",
    "\n",
    "      input_ids = d[\"input_ids\"].cuda()\n",
    "      attention_mask = d[\"attention_mask\"].cuda()\n",
    "      targets = d[\"targets\"].cuda()\n",
    "\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      all_targets.append(targets)\n",
    "      all_preds.append(preds)\n",
    "\n",
    "  all_targets = torch.cat(all_targets, dim=0)\n",
    "  all_preds = torch.cat(all_preds, dim=0)\n",
    "\n",
    "  return all_targets.cpu().numpy(), all_preds.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the F1-score from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.48      0.46       138\n",
      "           1       0.61      0.64      0.62       148\n",
      "           2       0.54      0.53      0.53       160\n",
      "           3       0.49      0.47      0.48       152\n",
      "           4       0.49      0.46      0.47       152\n",
      "\n",
      "    accuracy                           0.51       750\n",
      "   macro avg       0.51      0.51      0.51       750\n",
      "weighted avg       0.51      0.51      0.51       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "targets, preds = compare_predictions(model, test_data_loader)\n",
    "\n",
    "print(classification_report(targets, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the confusion matrix from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[66, 12, 12, 28, 20],\n",
       "       [18, 94, 23,  7,  6],\n",
       "       [15, 30, 84, 12, 19],\n",
       "       [25, 10, 18, 71, 28],\n",
       "       [27,  9, 18, 28, 70]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(targets, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and evaluate custom text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['😂', '😭', '😍', '😊', '😔']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_list_top5 = ['joy',\n",
    "            #'heart',\n",
    "            'sob',\n",
    "            'heart_eyes',\n",
    "            #'hearts',\n",
    "            'blush',\n",
    "            #'two_hearts',\n",
    "            #'kissing_heart',\n",
    "            'pensive',\n",
    "            ]\n",
    "\n",
    "emoji_list_icons = []\n",
    "\n",
    "for emj in emoji_list_top5:\n",
    "    emoji_list_icons.append(emoji.emojize(\":\" + emj + \":\",use_aliases=True))\n",
    "\n",
    "emoji_list_icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28998, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (out): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_text_model = SentimentClassifier(df.groupby(['emoji']).ngroups, tokenizer)\n",
    "eval_text_model = eval_text_model.cuda()\n",
    "\n",
    "eval_text_model.load_state_dict(torch.load('best_model_state_balanced2.bin'))\n",
    "eval_text_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_custom(tweet):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "      tweet,\n",
    "      add_special_tokens=True,\n",
    "      max_length=200,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "      truncation=True,\n",
    "    )\n",
    "\n",
    "    eval_pred = eval_text_model(encoding['input_ids'].cuda(), encoding['attention_mask'].cuda())\n",
    "\n",
    "    return emoji_list_icons[torch.argmax(eval_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😭'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"Today is a great day!\"\n",
    "predict_from_custom(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😔'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"I hate mondays so much\"\n",
    "predict_from_custom(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😍'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"Love my entire family\"\n",
    "predict_from_custom(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😔'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"omg its raining again\"\n",
    "predict_from_custom(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😭'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"I live in Vancouver\"\n",
    "predict_from_custom(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😊'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"I live in America\"\n",
    "predict_from_custom(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😊'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"I live in Canada\"\n",
    "predict_from_custom(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😊'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"Is it the weekend yet?\"\n",
    "predict_from_custom(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😔'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"Im a student at SFU, in the fourth year, \\\n",
    "        trying to get a good grade in my NLP class \\\n",
    "        and hopefully get accepted into a master program. \\\n",
    "        I feel like Im constantly working\"\n",
    "predict_from_custom(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "328e0d9221294a5780d403799031349f4dd3c3c18eb365adbfaa4af25cc5badf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
